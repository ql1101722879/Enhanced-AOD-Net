class AODnet_HDConv(nn.Module):
    def __init__(self):
        super(AODnet_HDConv, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1, padding=0)
        self.conv2_1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, dilation=1)
        self.conv2_2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=2, dilation=2)
        self.conv2_3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=3, dilation=3)
        self.conv3_1 = nn.Conv2d(in_channels=9, out_channels=3, kernel_size=5, stride=1, padding=2, dilation=1)
        self.conv3_2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=1, padding=4, dilation=2)
        self.conv3_3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5, stride=1, padding=6, dilation=3)
        self.conv4_1 = nn.Conv2d(in_channels=21, out_channels=3, kernel_size=7, stride=1, padding=3, dilation=1)
        self.conv4_2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=7, stride=1, padding=6, dilation=2)
        self.conv4_3 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=7, stride=1, padding=9, dilation=3)
        self.conv5 = nn.Conv2d(in_channels=45, out_channels=3, kernel_size=3, stride=1, padding=1)
        # self.s2A1 = S2Attention(channels=6, spl_channels=6)
        # self.s2A2 = S2Attention(channels=9, spl_channels=9)
        self.s2A3 = S2Attention(channels=45, spl_channels=45)
        self.b = 1

    def forward(self, x):
        # print(x.size())
        x1 = F.relu(self.conv1(x))   # 3
        # print(x1.size())   # 3 640 480
        x2_1 = F.relu(self.conv2_1(x1))  # 3
        # print(x2_1.size()) # 3 640 480
        x2_2 = F.relu(self.conv2_2(x2_1))
        # print(x2_2.size()) # 3 640 480
        x2_3 = F.relu(self.conv2_3(x2_2))
        # print(x2_3.size()) # 3 640 480
        x2 = torch.cat((x1, x2_3), 1)
        # print(x2.size()) # 6 640 480
        cat1 = torch.cat((x1, x2), 1) # 6
        # print(cat1.size()) # 9 640 480
        x3_1 = F.relu(self.conv3_1(cat1)) # 3
        # print(x3_1.size()) # 3 640 480
        x3_2 = F.relu(self.conv3_2(x3_1))
        # print(x3_2.size()) # 3 640 480
        x3_3 = F.relu(self.conv3_3(x3_2))
        # print(x3_3.size()) # 3 640 480
        x3 = torch.cat((cat1, x3_3), 1)
        # print(x3.size())  # 12 640 480
        cat2 = torch.cat((x1, x2, x3), 1) # 9
        # print(cat2.size()) # 21 640 480
        x4_1 = F.relu(self.conv4_1(cat2)) # 3
        # print(x4_1.size()) # 3 640 480
        x4_2 = F.relu(self.conv4_2(x4_1))
        # print(x4_2.size()) # 3 640 480
        x4_3 = F.relu(self.conv4_3(x4_2))
        # print(x4_3.size()) # 3 640 480
        x4 = torch.cat((cat2, x4_3), 1)
        # print(x4.size()) # 24 640 480
        cat3 = torch.cat((x1, x2, x3, x4), 1) # 12
        # print(cat3.size()) # 45 640 480
        cat3 = self.s2A3(cat3)
        # print(cat3.size())  # 45 640 480
        k = F.relu(self.conv5(cat3))

        if k.size() != x.size():
            raise Exception("k, haze image are different size!")

        output = k * x - k + self.b
        return F.relu(output)

class AODnet_HDCS2(nn.Module):
    def __init__(self):
        super(AODnet_HDCS2, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1, padding=0)
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, dilation=1)
        self.conv3 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=5, stride=1, padding=4, dilation=2)
        self.conv4 = nn.Conv2d(in_channels=9, out_channels=3, kernel_size=7, stride=1, padding=9, dilation=3)
        self.conv5 = nn.Conv2d(in_channels=12, out_channels=3, kernel_size=3, stride=1, padding=1)
        self.b = 1
        self.s2A3 = S2Attention(channels=12, spl_channels=12)

    def forward(self, x):
        x1 = F.relu(self.conv1(x))  # torch.Size([2, 3, 640, 480])
        # print(x1.size())
        x2 = F.relu(self.conv2(x1)) # torch.Size([2, 3, 640, 480])
        # print(x2.size())
        cat1 = torch.cat((x1, x2), 1) # torch.Size([2, 6, 640, 480])
        # print(cat1.size())
        x3 = F.relu(self.conv3(cat1)) # torch.Size([2, 3, 640, 480])
        # print(x3.size())
        cat2 = torch.cat((x1, x2, x3), 1) # torch.Size([2, 9, 640, 480])
        # print(cat2.size())
        x4 = F.relu(self.conv4(cat2)) # torch.Size([2, 3, 640, 480])
        # print(x4.size())
        cat3 = torch.cat((x1, x2, x3, x4), 1) # torch.Size([2, 12, 640, 480])
        # print(cat3.size())
        cat3 = self.s2A3(cat3)
        k = F.relu(self.conv5(cat3))

        if k.size() != x.size():
            raise Exception("k, haze image are different size!")

        output = k * x - k + self.b
        return F.relu(output)

def spatial_shift1(x):
    b, w, h, c = x.size()
    x[:, 1:, :, :c // 4] = x[:, :w - 1, :, :c // 4]
    x[:, :w - 1, :, c // 4:c // 2] = x[:, 1:, :, c // 4:c // 2]
    x[:, :, 1:, c // 2:c * 3 // 4] = x[:, :, :h - 1, c // 2:c * 3 // 4]
    x[:, :, :h - 1, 3 * c // 4:] = x[:, :, 1:, 3 * c // 4:]
    return x


def spatial_shift2(x):
    b, w, h, c = x.size()
    x[:, :, 1:, :c // 4] = x[:, :, :h - 1, :c // 4]
    x[:, :, :h - 1, c // 4:c // 2] = x[:, :, 1:, c // 4:c // 2]
    x[:, 1:, :, c // 2:c * 3 // 4] = x[:, :w - 1, :, c // 2:c * 3 // 4]
    x[:, :w - 1, :, 3 * c // 4:] = x[:, 1:, :, 3 * c // 4:]
    return x


class SplitAttention(nn.Module):
    def __init__(self, channel=6, k=3):
        super().__init__()
        self.channel = channel
        self.k = k
        self.mlp1 = nn.Linear(channel, channel, bias=False)
        self.gelu = nn.GELU()
        self.mlp2 = nn.Linear(channel, channel * k, bias=False)
        self.softmax = nn.Softmax(1)

    def forward(self, x_all):
        b, k, h, w, c = x_all.shape
        x_all = x_all.reshape(b, k, -1, c)
        a = torch.sum(torch.sum(x_all, 1), 1)
        hat_a = self.mlp2(self.gelu(self.mlp1(a)))
        hat_a = hat_a.reshape(b, self.k, c)
        bar_a = self.softmax(hat_a)
        attention = bar_a.unsqueeze(-2)
        out = attention * x_all
        out = torch.sum(out, 1).reshape(b, h, w, c)
        return out


class S2Attention(nn.Module):
    # # AIEAGNY
    def __init__(self, channels=512, spl_channels=512):
        super().__init__()
        self.mlp1 = nn.Linear(channels, channels * 3)
        self.mlp2 = nn.Linear(channels, channels)
        self.split_attention = SplitAttention(spl_channels)

    def forward(self, x):
        b, c, w, h = x.size()
        x = x.permute(0, 2, 3, 1)
        x = self.mlp1(x)
        x1 = spatial_shift1(x[:, :, :, :c])
        x2 = spatial_shift2(x[:, :, :, c:c * 2])
        x3 = x[:, :, :, c * 2:]
        x_all = torch.stack([x1, x2, x3], 1)
        a = self.split_attention(x_all)
        x = self.mlp2(a)
        x = x.permute(0, 3, 1, 2)
        return x